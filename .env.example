# ==============================================
# Legal AI System - Environment Configuration
# ==============================================
# Copy this file to .env and fill in your values

# ============================================
# Database Configuration
# ============================================
DATABASE_URL=postgresql://postgres:postgres123@localhost:5433/cases_llama3_3
DATABASE_HOST=localhost
DATABASE_PORT=5433
DATABASE_NAME=cases_llama3.3
DATABASE_USER=postgres
DATABASE_PASSWORD=postgres123

# Legacy PostgreSQL environment variables
PGHOST=localhost
PGPORT=5433
PGDATABASE=cases_llama3.3
PGUSER=postgres
PGPASSWORD=postgres123

# ============================================
# AI/LLM Configuration
# ============================================

# OpenAI Configuration (for embeddings and AI features)
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_MODEL=gpt-4o-mini

# Ollama Configuration (local LLM alternative)
# Install Ollama from: https://ollama.ai
OLLAMA_MODEL=qwen2.5:32b-instruct
OLLAMA_EMBED_MODEL=mxbai-embed-large
OLLAMA_BASE_URL=http://localhost:11434
USE_OLLAMA=true

# Other AI Services (optional)
PINECONE_API_KEY=
LANGCHAIN_API_KEY=

# ============================================
# Redis Configuration
# ============================================
REDIS_URL=redis://localhost:6379

# ============================================
# Server Configuration
# ============================================
API_V1_STR=/api/v1
SERVER_NAME=Law Helper API
SERVER_HOST=localhost
SERVER_PORT=8000
DEBUG=true

# Security
SECRET_KEY=your-secret-key-here-change-in-production
ACCESS_TOKEN_EXPIRE_MINUTES=30

# ============================================
# CORS Configuration
# ============================================
BACKEND_CORS_ORIGINS=["http://localhost:3000","http://localhost:8080"]

# ============================================
# Feature Flags
# ============================================
# Use OpenAI for embeddings (requires OPENAI_API_KEY)
USE_OPENAI_EMBEDDINGS=true

# Use Ollama as fallback (requires Ollama running locally)
USE_OLLAMA_FALLBACK=true
